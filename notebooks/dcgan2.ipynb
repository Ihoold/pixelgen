{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "funit.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3"
    },
    "accelerator": "TPU",
    "pycharm": {
      "stem_cell": {
        "cell_type": "raw",
        "source": [],
        "metadata": {
          "collapsed": false
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ihoold/pixelgen/blob/master/notebooks/dcgan2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y-KtZcaWB02O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.python.estimator import estimator\n",
        "from google.colab import drive, auth"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mi4fxG6GVo7D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "R, C = 4, 3\n",
        "EXAMPLES = R * C\n",
        "    \n",
        "CHANNELS = 4\n",
        "LATENT_DIM = 128\n",
        "ADD_NOISE_TO_EXAMPLE = False\n",
        "\n",
        "DROPOUT_PROB = 0.5\n",
        "ALPHA = 0.2\n",
        "BATCH_SIZE = 1024\n",
        "EPOCHS = 300000\n",
        "EVAL_EPOCHS = 5000\n",
        "G_LR = 0.0002\n",
        "D_LR = 0.0001\n",
        "KERNEL_SIZE = 4\n",
        "\n",
        "MODEL_NAME = 'DCGAN'\n",
        "RUN_NAME = 'DCGAN_25'\n",
        "\n",
        "data_file = 'gs://tputestingmnist/datasets/characters_front.tfrecords'\n",
        "MODEL_DIR = 'gs://tputestingmnist/{}/{}/'.format(MODEL_NAME, RUN_NAME)\n",
        "GOOGLE_DRIVE_DIR = '/content/gdrive/My Drive/Programowanie/PixelGen/{}'.format(RUN_NAME)\n",
        "TF_MASTER = 'grpc://{}'.format(os.environ['COLAB_TPU_ADDR'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CCzCaKLPB02f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#################################### SETUP #####################################\n",
        "\n",
        "def setup():\n",
        "    drive.mount('/content/gdrive')\n",
        "    auth.authenticate_user()\n",
        "\n",
        "\n",
        "def upload_credentials():\n",
        "    # Upload credentials to TPU.\n",
        "    with tf.Session(TF_MASTER) as sess:    \n",
        "        with open('/content/adc.json', 'r') as f:\n",
        "            auth_info = json.load(f)\n",
        "        tf.contrib.cloud.configure_gcs(sess, credentials=auth_info)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r2ycJsJCB02o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "################################# DATA INPUT ###################################\n",
        "\n",
        "def parser(serialized_example):\n",
        "        \"\"\"Parses a single example into image and label tensors.\"\"\"\n",
        "        features = tf.parse_single_example(\n",
        "            serialized_example,\n",
        "            features={\n",
        "                'image_raw': tf.FixedLenFeature([], tf.string),\n",
        "                'label': tf.FixedLenFeature([], tf.int64) \n",
        "            })\n",
        "\n",
        "        image = tf.decode_raw(features['image_raw'], tf.uint8)\n",
        "        image.set_shape([48 * 48 * 4])\n",
        "        image = tf.reshape(image, [48, 48, 4])[:,:,:CHANNELS]\n",
        "        # Normalize the values of the image from [0, 255] to [-1.0, 1.0]\n",
        "        image = tf.cast(image, dtype=tf.float32) / 127.5 - 1\n",
        "        return image\n",
        "\n",
        "    \n",
        "def make_input_fn(is_training=True):\n",
        "    def input_fn(params):\n",
        "        batch_size = params['batch_size']\n",
        "        dataset = tf.data.TFRecordDataset(data_file, buffer_size=8*1024*1024)\n",
        "        dataset = dataset.map(parser).cache().shuffle(batch_size)\n",
        "        if is_training:\n",
        "            dataset = dataset.repeat()\n",
        "            \n",
        "        images = dataset.batch(batch_size, drop_remainder=True).prefetch(8).make_one_shot_iterator().get_next()\n",
        "\n",
        "        if ADD_NOISE_TO_EXAMPLE:\n",
        "            images += tf.random_normal(shape=tf.shape(images), mean=0.0, stddev=0.1, dtype=tf.float32)\n",
        "\n",
        "        features = {\n",
        "            'images': images,\n",
        "            'noise': tf.random_uniform([params['batch_size'], LATENT_DIM], -1, 1, dtype=tf.float32)\n",
        "        }\n",
        "        return features, None\n",
        "    return input_fn\n",
        "\n",
        "\n",
        "def noise_input_fn(params):  \n",
        "    noise_dataset = tf.data.Dataset.from_tensors(tf.constant(np.random.uniform(-1, 1, (params['batch_size'], LATENT_DIM)), dtype=tf.float32))\n",
        "    return {'noise': noise_dataset.make_one_shot_iterator().get_next()}, None"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P6i0DewZB02v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "############################### DATA SAVEING ###################################\n",
        " \n",
        "def images_to_zero_one(images):\n",
        "        return np.clip(np.array(images) * 0.5 + 0.5, 0., 1.)\n",
        "\n",
        "\n",
        "def save_imgs(epoch, images):\n",
        "    if not os.path.exists(GOOGLE_DRIVE_DIR):\n",
        "        os.mkdir(GOOGLE_DRIVE_DIR)\n",
        "\n",
        "    # Rescale images to 0 - 1\n",
        "    images = images_to_zero_one(images)\n",
        "    fig, axs = plt.subplots(R, C, figsize=(20,20))\n",
        "\n",
        "    for i in range(R):\n",
        "        for j in range(C):\n",
        "            axs[i,j].imshow(images[C*i + j])\n",
        "            axs[i,j].axis('off')\n",
        "\n",
        "    fig.savefig(os.path.join(GOOGLE_DRIVE_DIR, '{}.png'.format(epoch)))\n",
        "    plt.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "khVvoOzKB024",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "################################## MODEL #######################################\n",
        "\n",
        "def _relu(x):\n",
        "    return tf.nn.relu(x)\n",
        "\n",
        "\n",
        "def _leaky_relu(x):\n",
        "    return tf.nn.leaky_relu(x, alpha=ALPHA)\n",
        "\n",
        "\n",
        "def _get_batch_norm(is_training, name):\n",
        "    def batch_norm(x):\n",
        "        return tf.layers.batch_normalization(x, momentum=0.8, epsilon=1e-5, \n",
        "                                             training=is_training, name=name)\n",
        "    return batch_norm\n",
        "\n",
        "\n",
        "def _dense(x, neurons, name, activation=None):\n",
        "    return tf.layers.dense(x, neurons, name=name, activation=activation,\n",
        "                           kernel_initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
        "\n",
        "\n",
        "def _conv2d(x, filters, kernel_size, stride, name, activation=None):\n",
        "    return tf.layers.conv2d(x, filters, [kernel_size, kernel_size], \n",
        "                            strides=[stride, stride], activation=activation,\n",
        "                            padding='same', name=name,\n",
        "                            kernel_initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
        "\n",
        "def _pooling(x, kernel, stride, index):\n",
        "    return tf.layers.average_pooling2d(x, pool_size=kernel, strides=stride, name='pool_{}'.format(index))\n",
        "\n",
        "def _deconv2d(x, filters, stride, name, activation=None):\n",
        "    return tf.layers.conv2d_transpose(x, filters, [KERNEL_SIZE, KERNEL_SIZE],\n",
        "                                      strides=[stride, stride], activation=activation,\n",
        "                                      padding='same', name=name,\n",
        "                                      kernel_initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
        "\n",
        "\n",
        "def convolution_block(x, filters, kernel_size, resize_factor, index, activation=_leaky_relu, activation_first=False, normalization=None):\n",
        "    if activation and activation_first:\n",
        "        x = activation(x)\n",
        "    x = _conv2d(x, kernel_size=kernel_size, filters=filters, stride=resize_factor, activation=None, name='conv_{}'.format(index))\n",
        "    if normalization:\n",
        "        x = normalization(x)\n",
        "    if activation and not activation_first:\n",
        "        x = activation(x)\n",
        "    return x\n",
        "\n",
        "\n",
        "def deconvolution_block(x, filters, resize_factor, index, activation=_relu, normalization=None):\n",
        "    x = _deconv2d(x, filters=filters, stride=resize_factor, activation=None, name='deconv_{}'.format(index))\n",
        "    if normalization:\n",
        "        x = normalization(x)\n",
        "    if activation:\n",
        "        x = activation(x)\n",
        "    return x\n",
        "\n",
        "\n",
        "class Funit:\n",
        "\n",
        "    @staticmethod\n",
        "    def discriminator(x, is_training=False, scope='Discriminator'):\n",
        "        with tf.variable_scope(scope, reuse=tf.AUTO_REUSE):\n",
        "            x = convolution_block(x, 64, kernel_size=KERNEL_SIZE, resize_factor=2, index='disc_11')\n",
        "            x = convolution_block(x, 128, kernel_size=KERNEL_SIZE, resize_factor=2, normalization=_get_batch_norm(is_training, 'bn11'), index='disc_12')\n",
        "            x = convolution_block(x, 256, kernel_size=KERNEL_SIZE, resize_factor=2, normalization=_get_batch_norm(is_training, 'bn12'), index='disc_13')\n",
        "            x = convolution_block(x, 512, kernel_size=KERNEL_SIZE, resize_factor=2, normalization=_get_batch_norm(is_training, 'bn13'), index='disc_14')\n",
        "            \n",
        "            output = convolution_block(x, 1, kernel_size=3, resize_factor=3, activation=None, index='disc_out')\n",
        "            return output, x\n",
        "          \n",
        "    @staticmethod\n",
        "    def generator(latent_code, is_training=False, scope='Generator'):\n",
        "        with tf.variable_scope(scope, reuse=tf.AUTO_REUSE):\n",
        "            x = _dense(latent_code, 1024 * 3 * 3, activation=tf.nn.relu, name='g_dense')\n",
        "            x = tf.reshape(x, [-1, 3, 3, 1024])\n",
        "            \n",
        "            x = deconvolution_block(x, 512, 2, index='gen_11', normalization=_get_batch_norm(is_training, 'bn21'))\n",
        "            x = deconvolution_block(x, 256, 2, index='gen_12', normalization=_get_batch_norm(is_training, 'bn22'))\n",
        "            x = deconvolution_block(x, 128, 2, index='gen_13', normalization=_get_batch_norm(is_training, 'bn23'))\n",
        "            x = deconvolution_block(x, CHANNELS, 2, index='gen_14', activation=tf.tanh)\n",
        "\n",
        "            return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tSBN3P9whXDn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# It's not exactly the norm, but taking mean instead of sum makes the losses more comparable\n",
        "def l1_norm(x):\n",
        "    return tf.reduce_mean(tf.math.abs(x), axis=[1,2,3])\n",
        "\n",
        "def l2_norm(x):\n",
        "    return tf.reduce_mean(tf.square(x), axis=[0,1,2])\n",
        "\n",
        "def pgd(model, x, y, index, epsilon=0.1, alpha=0.015, num_iter=5):\n",
        "    with tf.variable_scope('pgd_{}'.format(index)):\n",
        "        delta = tf.zeros_like(x, dtype=tf.float32)\n",
        "\n",
        "        false_condition = lambda d: False\n",
        "        def body(delta):\n",
        "            pred, _ = model(x + delta)\n",
        "            pred_s = tf.squeeze(pred)\n",
        "            loss = tf.losses.sigmoid_cross_entropy(y, pred_s)\n",
        "\n",
        "            optimizer = tf.train.GradientDescentOptimizer(learning_rate=None)\n",
        "            gradients = optimizer.compute_gradients(loss, [delta])\n",
        "            delta_grad = gradients[0][0]\n",
        "\n",
        "#             delta = tf.clip_by_value(delta + tf.math.sign(delta_grad) * alpha, -epsilon, epsilon)\n",
        "            return tf.clip_by_value(tf.math.sign(delta_grad) * alpha, -epsilon, epsilon)\n",
        "        \n",
        "        return x + tf.while_loop(false_condition, body, (delta,), maximum_iterations=num_iter)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gcr7UF8SB029",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "################################ MODEL FUN #####################################\n",
        "def make_model_fn(model):\n",
        "\n",
        "    def model_fn(features, labels, mode, params):\n",
        "        is_training = (mode == tf.estimator.ModeKeys.TRAIN)\n",
        "        \n",
        "        with tf.variable_scope('inputs'):\n",
        "            noise = features['noise']\n",
        "            \n",
        "        generated_images = model.generator(noise, is_training)\n",
        "        \n",
        "        # PREDICT #\n",
        "        if mode == tf.estimator.ModeKeys.PREDICT:\n",
        "            predictions = {'generated_images': generated_images}\n",
        "            return tf.contrib.tpu.TPUEstimatorSpec(mode=mode, predictions=predictions)\n",
        "        \n",
        "        with tf.variable_scope('inputs'):\n",
        "            images = features['images']\n",
        "        \n",
        "        # Labels\n",
        "        d_on_data_labels = tf.ones([images.shape[0]])\n",
        "        d_on_g_labels = tf.zeros([images.shape[0]])\n",
        "        \n",
        "        # Attacks\n",
        "        attacked_images = pgd(model.discriminator, images, d_on_data_labels, 1)\n",
        "        attacked_generated_images = pgd(model.discriminator, generated_images, d_on_g_labels, 2)\n",
        "        \n",
        "        # Discriminator loss\n",
        "        d_on_data_results, d_on_data_features = model.discriminator(attacked_images, is_training)\n",
        "        d_on_data_logits = tf.squeeze(d_on_data_results)\n",
        "       \n",
        "        d_on_g_results, d_on_g_features = model.discriminator(attacked_generated_images, is_training)\n",
        "        d_on_g_logits = tf.squeeze(d_on_g_results)\n",
        "        \n",
        "        with tf.variable_scope('losses'):\n",
        "            d_loss = tf.contrib.gan.losses.wargs.modified_discriminator_loss(\n",
        "                discriminator_real_outputs=d_on_data_logits,\n",
        "                discriminator_gen_outputs=d_on_g_logits,\n",
        "                label_smoothing=0.2,\n",
        "                reduction=tf.losses.Reduction.NONE,\n",
        "            )\n",
        "            d_loss_reduced = tf.reduce_mean(d_loss)\n",
        "\n",
        "            # Generator loss\n",
        "            g_loss = tf.contrib.gan.losses.wargs.modified_generator_loss(\n",
        "                discriminator_gen_outputs=d_on_g_logits,\n",
        "                reduction=tf.losses.Reduction.NONE\n",
        "            )\n",
        "            g_loss_feature_matching = l2_norm(tf.reduce_mean(d_on_g_features, axis=0) - tf.reduce_mean(d_on_data_features, axis=0))\n",
        "            g_loss_reduced = tf.reduce_mean(g_loss + g_loss_feature_matching)\n",
        "            \n",
        "        # TRAIN #\n",
        "        if mode == tf.estimator.ModeKeys.TRAIN:\n",
        "            \n",
        "            with tf.variable_scope('optimizer'):\n",
        "                d_optimizer = tf.train.AdamOptimizer(learning_rate=D_LR, beta1=0.5)\n",
        "                d_optimizer = tf.contrib.tpu.CrossShardOptimizer(d_optimizer)\n",
        "            \n",
        "                g_optimizer = tf.train.AdamOptimizer(learning_rate=G_LR, beta1=0.5)\n",
        "                g_optimizer = tf.contrib.tpu.CrossShardOptimizer(g_optimizer)\n",
        "         \n",
        "                with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)):\n",
        "                    d_step = d_optimizer.minimize(d_loss_reduced, var_list=tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES,\n",
        "                                                                                     scope='Discriminator'))\n",
        "                    g_step = g_optimizer.minimize(g_loss_reduced, var_list=tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES,\n",
        "                                                                                     scope='Generator'))\n",
        "                    increment_step = tf.assign_add(tf.train.get_or_create_global_step(), 1)\n",
        "                    joint_op = tf.group([d_step, g_step, increment_step])\n",
        "\n",
        "                    a = tf.contrib.tpu.TPUEstimatorSpec(mode=mode, loss=d_loss_reduced+g_loss_reduced, train_op=joint_op)\n",
        "                    return a\n",
        "\n",
        "        # EVAL #\n",
        "        elif mode == tf.estimator.ModeKeys.EVAL:\n",
        "            def _eval_metric_fn(d_loss, g_loss_1, d_real_labels, d_gen_lanels, d_real_logits, d_gen_logits):\n",
        "                return {\n",
        "                    'discriminator_loss': tf.metrics.mean(d_loss),\n",
        "                    'generator_loss': tf.metrics.mean(g_loss_1),\n",
        "                    'discriminator_real_accuracy': tf.metrics.accuracy(labels=d_real_labels, predictions=tf.math.round(tf.sigmoid(d_real_logits))),\n",
        "                    'discriminator_gen_accuracy': tf.metrics.accuracy(labels=d_gen_lanels, predictions=tf.math.round(tf.sigmoid(d_gen_logits))),\n",
        "                }\n",
        "\n",
        "            return tf.contrib.tpu.TPUEstimatorSpec(mode=mode, loss=d_loss_reduced + g_loss_reduced,\n",
        "                                                   eval_metrics=(_eval_metric_fn, [d_loss, g_loss, \n",
        "                                                                                   d_on_data_labels, d_on_g_labels,\n",
        "                                                                                   d_on_data_logits, d_on_g_logits]))\n",
        "    return model_fn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-5rb586jB03F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "################################ ESTIMATORS ####################################\n",
        "\n",
        "def make_estimators(model, only_cpu=False):\n",
        "    model_fn = make_model_fn(model)\n",
        "    \n",
        "    config = tf.contrib.tpu.RunConfig(\n",
        "        master=TF_MASTER,\n",
        "        save_checkpoints_steps=EVAL_EPOCHS,\n",
        "        save_checkpoints_secs=None,\n",
        "        save_summary_steps=EVAL_EPOCHS,\n",
        "        model_dir=MODEL_DIR,\n",
        "        keep_checkpoint_max=3,\n",
        "        tpu_config=tf.contrib.tpu.TPUConfig(iterations_per_loop=EVAL_EPOCHS))\n",
        "\n",
        "    if not only_cpu:\n",
        "        # TPU-based estimator used for TRAIN and EVAL\n",
        "        est = tf.contrib.tpu.TPUEstimator(\n",
        "            model_fn=model_fn,\n",
        "            use_tpu=True,\n",
        "            config=config,\n",
        "            train_batch_size=BATCH_SIZE,\n",
        "            eval_batch_size=BATCH_SIZE)\n",
        "    else:\n",
        "        est = None\n",
        "\n",
        "    # CPU-based estimator used for PREDICT (generating images)\n",
        "    cpu_est = tf.contrib.tpu.TPUEstimator(\n",
        "        model_fn=model_fn,\n",
        "        use_tpu=False,\n",
        "        config=config,\n",
        "        predict_batch_size=EXAMPLES)\n",
        "    \n",
        "    return est, cpu_est"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7nIuQqdWB03O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "################################# TRAINING #####################################\n",
        "\n",
        "def train(est, cpu_est):\n",
        "    current_step = estimator._load_global_step_from_checkpoint_dir(MODEL_DIR)\n",
        "    print('Starting training')\n",
        "\n",
        "    while current_step < EPOCHS:\n",
        "        next_checkpoint = int(min(current_step + EVAL_EPOCHS, EPOCHS))\n",
        "        est.train(input_fn=make_input_fn(), max_steps=next_checkpoint)\n",
        "        current_step = next_checkpoint\n",
        "        print('Finished training step %d' % current_step)\n",
        "\n",
        "        # Evaluation\n",
        "        metrics = est.evaluate(input_fn=make_input_fn(False), steps=1)\n",
        "        print('Finished evaluating')\n",
        "        print(metrics)\n",
        "\n",
        "        # Render some generated images\n",
        "        generated_iter = cpu_est.predict(input_fn=noise_input_fn)\n",
        "        images = [p['generated_images'] for p in generated_iter]\n",
        "        save_imgs(str(current_step), images)\n",
        "        print('Finished generating images')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "STjYktznB03T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def do_experiment():\n",
        "    setup()\n",
        "    upload_credentials()\n",
        "    model = Funit()\n",
        "    est, cpu_est = make_estimators(model)\n",
        "    train(est, cpu_est)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3eImay3kpN2C",
        "colab_type": "code",
        "outputId": "92d6a120-6f20-4a83-c2cb-f9cd02b32e3f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 882
        }
      },
      "source": [
        "tf.logging.set_verbosity(tf.logging.INFO)\n",
        "\n",
        "try:\n",
        "    do_experiment()\n",
        "except Exception as e:\n",
        "    print (e)\n",
        "    pass"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0829 17:40:48.027715 140254910023552 estimator.py:1984] Estimator's model_fn (<function make_model_fn.<locals>.model_fn at 0x7f8f59872e18>) includes params argument, but params are not passed to Estimator.\n",
            "I0829 17:40:48.029333 140254910023552 estimator.py:209] Using config: {'_model_dir': 'gs://tputestingmnist/DCGAN/DCGAN_25/', '_tf_random_seed': None, '_save_summary_steps': 5000, '_save_checkpoints_steps': 5000, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true\n",
            "graph_options {\n",
            "  rewrite_options {\n",
            "    meta_optimizer_iterations: ONE\n",
            "  }\n",
            "}\n",
            ", '_keep_checkpoint_max': 3, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': None, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f8f5b247f60>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': 'grpc://10.25.164.18:8470', '_evaluation_master': 'grpc://10.25.164.18:8470', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1, '_tpu_config': TPUConfig(iterations_per_loop=5000, num_shards=None, num_cores_per_replica=None, per_host_input_for_training=2, tpu_job_name=None, initial_infeed_sleep_secs=None, input_partition_dims=None, eval_training_input_configuration=2), '_cluster': None}\n",
            "I0829 17:40:48.030497 140254910023552 tpu_context.py:209] _TPUContext: eval_on_tpu True\n",
            "W0829 17:40:48.031557 140254910023552 estimator.py:1984] Estimator's model_fn (<function make_model_fn.<locals>.model_fn at 0x7f8f59872e18>) includes params argument, but params are not passed to Estimator.\n",
            "I0829 17:40:48.033343 140254910023552 estimator.py:209] Using config: {'_model_dir': 'gs://tputestingmnist/DCGAN/DCGAN_25/', '_tf_random_seed': None, '_save_summary_steps': 5000, '_save_checkpoints_steps': 5000, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true\n",
            "graph_options {\n",
            "  rewrite_options {\n",
            "    meta_optimizer_iterations: ONE\n",
            "  }\n",
            "}\n",
            ", '_keep_checkpoint_max': 3, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': None, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f8f5b2475c0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': 'grpc://10.25.164.18:8470', '_evaluation_master': 'grpc://10.25.164.18:8470', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1, '_tpu_config': TPUConfig(iterations_per_loop=5000, num_shards=None, num_cores_per_replica=None, per_host_input_for_training=2, tpu_job_name=None, initial_infeed_sleep_secs=None, input_partition_dims=None, eval_training_input_configuration=2), '_cluster': None}\n",
            "I0829 17:40:48.034521 140254910023552 tpu_context.py:209] _TPUContext: eval_on_tpu True\n",
            "W0829 17:40:48.035764 140254910023552 tpu_context.py:211] eval_on_tpu ignored because use_tpu is False.\n",
            "I0829 17:40:48.244779 140254910023552 tpu_system_metadata.py:78] Querying Tensorflow master (grpc://10.25.164.18:8470) for TPU system metadata.\n",
            "I0829 17:40:48.249861 140254910023552 tpu_system_metadata.py:148] Found TPU system:\n",
            "I0829 17:40:48.250988 140254910023552 tpu_system_metadata.py:149] *** Num TPU Cores: 8\n",
            "I0829 17:40:48.251998 140254910023552 tpu_system_metadata.py:150] *** Num TPU Workers: 1\n",
            "I0829 17:40:48.252943 140254910023552 tpu_system_metadata.py:152] *** Num TPU Cores Per Worker: 8\n",
            "I0829 17:40:48.253917 140254910023552 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:CPU:0, CPU, -1, 3443979333535014340)\n",
            "I0829 17:40:48.254889 140254910023552 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, 3466908720739026821)\n",
            "I0829 17:40:48.255922 140254910023552 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, 13554240143092978346)\n",
            "I0829 17:40:48.256933 140254910023552 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, 17857708986354057537)\n",
            "I0829 17:40:48.258355 140254910023552 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, 6436864085196344498)\n",
            "I0829 17:40:48.259588 140254910023552 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 3709367861011979143)\n",
            "I0829 17:40:48.261053 140254910023552 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, 11001569486905880379)\n",
            "I0829 17:40:48.261979 140254910023552 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 11711789466084575079)\n",
            "I0829 17:40:48.262876 140254910023552 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, 2975808243373214197)\n",
            "I0829 17:40:48.263759 140254910023552 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 8589934592, 9264889175306998691)\n",
            "I0829 17:40:48.264672 140254910023552 tpu_system_metadata.py:154] *** Available Device: _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 1034242129324549655)\n",
            "I0829 17:40:48.289327 140254910023552 estimator.py:1145] Calling model_fn.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Starting training\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0829 17:40:52.854279 140254910023552 basic_session_run_hooks.py:541] Create CheckpointSaverHook.\n",
            "I0829 17:40:52.968006 140254910023552 estimator.py:1147] Done calling model_fn.\n",
            "I0829 17:40:52.969333 140254910023552 tpu_estimator.py:499] TPU job name tpu_worker\n",
            "I0829 17:40:53.346903 140254910023552 monitored_session.py:240] Graph was finalized.\n",
            "E0829 17:40:53.824764 140254910023552 error_handling.py:70] Error recorded from training_loop: 738 nodes in a cycle\n",
            "I0829 17:40:53.828240 140254910023552 error_handling.py:96] training_loop marked as finished\n",
            "W0829 17:40:53.829436 140254910023552 error_handling.py:130] Reraising captured error\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "738 nodes in a cycle\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}